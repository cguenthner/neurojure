(ns neurojure.core
  (:require [clojure.set :as set]
            [neurojure.utils :as u :refer [def- -?> -?>>]]
            [ranvier.core :as r :refer [G]]
            [tensure.core :as m]))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Initializers
;;
;; Initializers are functions that take a single argument, a tensor shape, and return a tensor of that shape,
;; with elements set to values determined by the initializer. Except for `zeros` and `ones`, the initializers
;; are generated by factory functions that return initializers with properties based on the parameters they
;; receive.
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

; TODO: Test this
(defn- get-axis-size
  "Given a vector representing a tensor `shape` and either a single `axis` index or vector of axis indices,
  returns the product of the sizes of the specified axes."
  [shape axis]
  (let [axes (if (number? axis)
               [axis]
               axis)
        rank (count shape)]
    (reduce (fn [size i]
              (when (>= i rank)
                (u/throw-str "Invalid axis for a tensor of shape '" shape "'."))
              (* size (nth shape i)))
            1
            axes)))

(defn zeros-initializer
  "Returns a tensor of `shape` filled with 0's."
  [shape]
  (m/zeros shape))

(defn ones-initializer
  "Returns a tensor of `shape` filled with 1's."
  [shape]
  (m/ones shape))

(defn make-fill-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with `constant`."
  [constant]
  (fn [shape]
    (m/filled shape constant)))

(defn make-constant-initializer
  "Returns a function that takes a shape and will return the tensor `constant` of that shape. Throws an
  exception if the constant does not have the shape."
  [constant]
  (let [constant (if (m/array? constant)
                   constant
                   (m/array constant))
        constant-shape (m/shape constant)]
    (fn [shape]
      (when (not= constant-shape shape)
        (u/throw-str "Attempt to initialize a tensor of shape '" shape "' with a constant of shape '"
                     constant-shape "'."))
      constant)))

(defn make-uniform-random-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with random
  values uniformly distributed in [min, max). `min` and `max` can be either numbers or functions that when
  applied to the shape will return numbers."
  ([]
   (make-uniform-random-initializer -0.01 0.01))
  ([min max]
   (fn [shape]
     (let [nd (m/sample-uniform shape)
           min (m/array (if (fn? min) (min shape) min))
           max (m/array (if (fn? max) (max shape) max))]
       (m/add (m/mul (m/sub max min) nd) min)))))

(defn make-normal-random-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with random
  values normally distributed around `mean` with standard deviation `stdev`. `mean` and `stdev` can be either
  numbers or functions that return the numbers when applied to the shape."
  ([]
   (make-normal-random-initializer 0 0.01))
  ([mean stdev]
   (fn [shape]
     (let [nd (m/sample-normal shape)
           stdev (m/array (if (fn? stdev) (stdev shape) stdev))
           mean (m/array (if (fn? mean) (mean shape) mean))]
       (m/add (m/mul stdev nd) mean)))))

(defn make-truncated-random-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with random
  values normally distributed around `mean` with standard deviation `stdev` but with no values >2 SD's from
  the mean. `mean` and `stdev` can be either numbers or functions that return the numbers when applied to
  the shape."
  ([]
   (make-truncated-random-initializer 0 0.01))
  ([mean stdev]
   (fn [shape]
     (let [stdev (m/array (if (fn? stdev) (stdev shape) stdev))
           mean (m/array (if (fn? mean) (mean shape) mean))
           result-element-count (apply * shape)
           get-element-seq (fn next [batch-size]
                             (lazy-cat (m/eseq (m/sample-normal [batch-size]))
                                       (next (max 128 (* batch-size 0.05)))))
           result (->> (get-element-seq result-element-count)
                       (filter #(and (> % -2) (< % 2)))
                       (take result-element-count)
                       m/array)]
       (m/mul! result stdev)
       (m/add! result mean)
       (m/reshape result shape)))))

(defn make-variance-scaling-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with random
  values normally distributed around 0. The standard deviation will be `sqrt(scale / axis-sum)` where
  `axis-sum` is the sum of the sizes of all axes in `axis`. `axis` can be either a vector of axes or an
  integer indicating a single axis. The distribution is truncted such that no values >2 SD's from 0 are
  included."
  ([]
   (make-variance-scaling-initializer 1))
  ([scale]
   (make-variance-scaling-initializer scale 1))
  ([scale axis]
   (let [axes (if (number? axis) [axis] axis)]
     (fn [shape]
       (for [axis axes]
         (when (>= axis (count shape))
           (u/throw-str "Cannot initialize tensor of shape " shape " with variance scaled on axis "
                        axis ", because axis is out of bounds.")))
       ((make-truncated-random-initializer
          0
          (Math/sqrt (/ scale (apply + (map #(nth shape %) axes)))))
        shape)))))

(defn make-lecun-uniform-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with values
  uniformly distributed in `[-sqrt(3 / m), sqrt(3 / m))`, where m is the size of the `input-axes`, either
  a single axis index or vector of axis indices (defaults to 1)."
  ([]
   (make-lecun-uniform-initializer 1))
  ([input-axes]
   (let [bound-fn #(Math/sqrt (/ 3 (get-axis-size % input-axes)))]
     (make-uniform-random-initializer
       #(- (bound-fn %))
       bound-fn))))

(defn make-lecun-normal-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with values
  drawn from a truncated normal distribution with SD `sqrt(1 / m)`, where m is the size of the `input-axes`,
  either a single axis index or vector of axis indices (defaults to 1).  The distribution is truncated
  at +/- 2 SD's."
  ([]
   (make-lecun-normal-initializer 1))
  ([input-axes]
   (make-truncated-random-initializer
     0
     #(Math/sqrt (/ 1 (get-axis-size % input-axes))))))

(defn make-he-uniform-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with values
  uniformly distributed in `[-sqrt(6 / m), sqrt(6 / m))`, where m is the size of the `input-axes`,
  either a single axis index or vector of axis indices (defaults to 1)."
  ([]
   (make-he-uniform-initializer 1))
  ([input-axes]
   (let [bound-fn #(Math/sqrt (/ 6 (get-axis-size % input-axes)))]
     (make-uniform-random-initializer
       #(- (bound-fn %))
       bound-fn))))

(defn make-he-normal-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with values
  drawn from a truncated normal distribution with SD `sqrt(2 / m)`, where m is the size of the `input-axes`,
  either a single axis index or vector of axis indices (defaults to 1). The distribution is truncated at
  +/- 2 SD's."
  ([]
   (make-he-normal-initializer 1))
  ([input-axes]
   (make-truncated-random-initializer
     0
     #(Math/sqrt (/ 2 (get-axis-size % input-axes))))))

(defn make-xavier-uniform-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with values
  uniformly distributed in `[-sqrt(6 / (m + n)), sqrt(6 / (m + n)))`, where m and n are the sizes of the
  `input-axes` and `output-axes`, respectively (defaults to 1 and 0)."
  ([]
   (make-xavier-uniform-initializer 1 0))
  ([input-axes output-axes]
   (let [bound-fn #(Math/sqrt (/ 6 (+ (get-axis-size % input-axes) (get-axis-size % output-axes))))]
     (make-uniform-random-initializer
       #(- (bound-fn %))
       bound-fn))))

(defn make-xavier-normal-initializer
  "Returns a function that when applied to a shape will return a tensor of that shape filled with values
  drawn from a truncated normal distribution with SD `sqrt(2 / (m + n))`, where m and n are the sizes of the
  `input-axes` and `output-axes`, respectively (defaults to 1 and 0). The distribution is truncated
  at +/- 2 SD's."
  ([]
   (make-xavier-normal-initializer 1 0))
  ([input-axes output-axes]
   (make-truncated-random-initializer
     0
     #(Math/sqrt (/ 2 (+ (get-axis-size % input-axes) (get-axis-size % output-axes)))))))

; TODO: Test this.
(defn- get-layer-initializer
  "Returns an initializer (function that takes a shape and returns an initialized tensor of that shape),
  given:

    - `initializer-selector` - an initializer function or keyword
    - `input-axes` - the axes of the tensor being initialized that span 'input values' (either a single
      axis index or vector of axis indices or `nil` if this does not apply)
    - `output-axes` - the axes of the tensor being initialized that span 'output values' (either a single
      axis index or vector of axis indices or `nil` if this does not apply)
    - `layer-type` - keyword or string identifying the layer (used to provide more descriptive error
      message)

  If `initializer-selector` is a function, it's just returned. If it's a keyword, then the corresponding
  factory function is looked up and used to construct a new initializer. The product of the sizes of the
  `input-axes` should correspond to the 'fan in' of the layer (the number of input values that contribute to
  any single value in the output), while the product of the sizes of the `output-axes` should correspond to
  the 'fan out' of the layer (the number of output values that contribute to any single input value to the
  next layer). If the notion of 'fan in' or 'fan out' does not apply to a layer or cannot be calculated
  from the parameter shape, then `input-axes` or `output-axes` can be set to `nil`. Selecting an initializer
  that does not apply to a layer will cause an Exception."
  [initializer-selector input-axes output-axes layer-type]
  (if (fn? initializer-selector)
    initializer-selector
    (let [throw-invalid-initializer #(u/throw-str "Invalid initializer for '" layer-type "' layer: '"
                                                  initializer-selector "'.")]
      (when (and (nil? input-axes) (#{:lecun-uniform :luecn-normal :he-uniform :he-normal
                                      :xavier-uniform :savier-normal} initializer-selector))
        (throw-invalid-initializer))
      (when (and (nil? output-axes) (#{:xavier-uniform :savier-normal} initializer-selector))
        (throw-invalid-initializer))
      (case initializer-selector
        :zeros zeros-initializer
        :ones ones-initializer
        :uniform (make-uniform-random-initializer)
        :normal (make-normal-random-initializer)
        :truncated (make-truncated-random-initializer)
        :variance-scaling (make-variance-scaling-initializer)
        :lecun-uniform (make-lecun-uniform-initializer input-axes)
        :lecun-normal (make-lecun-normal-initializer input-axes)
        :he-uniform (make-he-uniform-initializer input-axes)
        :he-normal (make-he-normal-initializer input-axes)
        :xavier-uniform (make-xavier-uniform-initializer input-axes output-axes)
        :xavier-normal (make-xavier-normal-initializer input-axes output-axes)
        (throw-invalid-initializer)))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Layer helpers
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

; A map of layer type (a string) to the number of times `gen-layer-name` has been called with that string
; as the `layer-type` argument.
(def- layer-type-counters (atom {}))
(defn- gen-layer-name
  "Given `layer-type` (a keyword or string), returns a new unique keyword identifier for a layer of that type.
  E.g. `(gen-layer-name :dense)` will return 'dense0', `:dense1', ':dense2', etc. on each call."
  [layer-type]
  (let [type-str (name layer-type)]
    (if (contains? @layer-type-counters type-str)
      (swap! layer-type-counters update type-str inc)
      (swap! layer-type-counters assoc type-str 0))
    (keyword (str type-str (get @layer-type-counters type-str)))))

(defn- gen-param-name
  "Given a `layer-name` and a `param-name` (both either keywords or strings), returns a unique keyword
  identifier for the parameter. E.g. `(gen-param-name 'dense0' :W)` returns 'dense0_W'."
  [layer-name param-name]
  (if layer-name
    (keyword (str (name layer-name) "_" (name param-name)))
    param-name))

(defn- tag-layer-nodes-recursive
  [g input-set layer-name]
  (let [operands (r/get-node-operands g)]
    (if (input-set g)
      g
      (->> (mapv #(tag-layer-nodes-recursive % input-set layer-name) operands)
           (r/update-node g
                          :data (update (r/get-node-data g) :layers #(conj (or % []) layer-name))
                          :operands)))))

(defn- tag-layer-nodes
  "Traverses computation graph `g`, conj'ing `layer-name` into the :layers vector in the node's :data but
  ommitting all nodes in `input-nodes` (seq of nodes or any object that can be constructed into a node) and
  all their descendents."
  [g input-nodes layer-name]
  (let [input-set (->> (map r/construct-operand input-nodes)
                       (into #{}))
        node-data (r/get-node-data g)]
    (r/update-node (tag-layer-nodes-recursive g input-set layer-name)
                   :data (assoc node-data
                           :layers (conj (get node-data :layers []) layer-name)
                           :entry-to-layer layer-name))))

(defn- create-layer
  [layer-name input-nodes g]
  (tag-layer-nodes g input-nodes layer-name))

(defn- make-input-node
  "Returns an input node for a layer, given:

    - `input-name` - the keyword name of the input
    - `layer-name` - the keyword name of the layer
    - `initializer` - an initializer function, which must receive a map of node names to values at the time
      of initialization and return the initialized value of the node
    - `data` - an arbitrary map of additional data

  The values for layer inputs must be either 1) provided (e.g. passed in from a model's data), or 2)
  generated by the initialization function. For non-parameter/state inputs whose values are not provided,
  the node will be re-initialized with every iteration. For inputs that should be optimized, use
  `make-parameter-node`, and for inputs whose values should persist from iteration to iteration, use
  `make-state-node`."
  ([input-name layer-name]
   (make-input-node input-name layer-name nil))
  ([input-name layer-name initializer]
   (make-input-node input-name layer-name initializer {}))
  ([input-name layer-name initializer data]
   (r/input (gen-param-name layer-name input-name) initializer (assoc data
                                                                 :parameter false
                                                                 :trainable false))))

(defn- make-parameter-node
  "Like `make-input-node`, but returns a node for a parameter input. Parameters are passed in the `param-map`
  to the optimizer--their values can be changed by the optimizer with every iteration."
  ([param-name layer-name]
   (make-parameter-node param-name layer-name nil))
  ([param-name layer-name initializer]
   (make-parameter-node param-name layer-name initializer {}))
  ([param-name layer-name initializer data]
   (r/input (gen-param-name layer-name param-name) initializer (assoc data
                                                                 :parameter true
                                                                 :trainable true))))
(defn- make-state-node
  "Like `make-input-node`, but returns an input node for a 'state' input--i.e. a value that can change
  during an iteration and whose values are persisted between iterations."
  ([param-name layer-name]
   (make-state-node param-name layer-name nil))
  ([param-name layer-name initializer]
   (make-state-node param-name layer-name initializer {}))
  ([param-name layer-name initializer data]
   (r/input (gen-param-name layer-name param-name) initializer (assoc data
                                                                 :parameter true
                                                                 :trainable false))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Regularizers
;;
;; Regulariers are functions that receive two arguments:
;;   - a computation graph node that is assumed to be cost (though in practice it can be any node)
;;   - a parameter node for which to apply regularization
;; They must return a new computation graph node with regularization for the given parameter applied. They
;; are generally constructed using a make-* function that returns a regularizer with properties specified
;; by the arguments to the make-* function.
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

(defn make-l2-regularizer
  "Returns an L2 regularizer:

    `cost + regularization-weight / 2m * (sum of squares of all elements in parameter tensor)`

  where `m` is the number of training examples included in the cost calculation."
  ([num-examples]
   (make-l2-regularizer num-examples 0.001))
  ([num-examples regularization-weight]
   (fn [g parameter-node]
     (create-layer
       (gen-layer-name :l2-regularizer)
       [g num-examples parameter-node :predicting]
       (r/assoc-node-data
         (G (tensor-if :predicting
                       g
                       (+ g (* (/ regularization-weight (* 2 num-examples))
                               (esum (pow parameter-node 2)))
                          #_(report (make-value-reporter "reg: ") (make-print-logger)))))
         :regularizer-for (r/get-node-name parameter-node))))))

(defn make-l1-regularizer
  "Returns an L1 regularizer:

    `cost + regularization-weight / 2m * (sum of absolute value of all elements in parameter tensor)`

  where `m` is the number of training examples included in the cost calculation."
  ([num-examples]
   (make-l1-regularizer num-examples 0.001))
  ([num-examples regularization-weight]
   (fn [g parameter-node]
     (create-layer
       (gen-layer-name :l1-regularizer)
       [g parameter-node num-examples :predicting]
       (r/assoc-node-data
         (G (tensor-if :predicting
                       g
                       (+ g (* (/ regularization-weight (* 2 num-examples))
                               (esum (abs parameter-node))))))
         :regularizer-for (r/get-node-name parameter-node))))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Layers
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

;;
;; Layers
;;

(defn regularize
  [g]
  (->> (r/get-all-graph-nodes g)
       distinct
       (reduce (fn [regularized-graph node]
                 (if-let [regularizer (->> node r/get-node-data :regularizer)]
                   (regularizer regularized-graph node)
                   regularized-graph))
               g)))

(defn sigmoid-cross-entropy
  "Computes the sum over all elements of the result of
  `max(predicted, 0) - predicted * actual + log (1 + e^-abs(predicted))`. This is roughly equivalent to the
  total logistic loss `actual * -log(sigmoid(predicted)) + (1 - actual) * -log(1 - sigmoid(predicted))`, but
  with some modifications for numerical stability. Based on the TensorFlow implementation
  (https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)."
  [predicted actual]
  (G (esum
       (+ (max predicted 0)
          (* (- predicted) actual)
          (log (+ 1 (exp (- (abs predicted)))))))))

; TODO: Add axis option.
(defn dropout
  "Masks a fraction of elements in `g` and scales up remaining elements to maintain a constant sum:

    - `:frequency` (required) - frequency of elements to drop out (on average)
    - `:exact` (default true) - boolean indicating whether scaling of other options in `g` must be exact
      (i.e. proportional to the number of elements actually dropped out, which may vary randomly from run to
      run) or approximate (proportional to `:frequency`). The results for `:exact == true` and
      `:exact == false` converge when the size of `g` is large, but the `:exact` computation is more
      expensive."
  [g options]
  (u/check-key-validity options [:frequency] [:exact])
  (let [frequency (:frequency options)
        exact (if (contains? options :exact)
                (:exact options)
                true)
        keep-vec (G (>= (random-uniform (shape g))
                        frequency))
        scaling-factor (if exact
                         (G (/ (size g)
                               (esum keep-vec)))
                         (/ 1 (- 1 frequency)))]
    (create-layer
      (gen-layer-name :dropout)
      [g :training]
      (G (tensor-if :training
                    (* keep-vec g scaling-factor)
                    g)))))

(defn mean
  "Calculates the arithmetic mean of `g` along some axes/axes. `options` can include:

    - `:axis` (optional) - a scalar or vector indicating the axes across which the mean should be taken; if
      ommitted, the mean of all elements in `g` will be returned.
    - `:collapse` (optional) - boolean indicating if the dimensions specified in `:axis` should be removed.
      If false, dimensions will be maintained with a size of 1."
  ([g]
   (mean g {}))
  ([g options]
   (u/check-key-validity options [] [:axis :collapse])
   (let [{:keys [axis collapse]} options]
     (create-layer
       (gen-layer-name :mean)
       [g]
       (G (/ (sum-along g {:axis axis
                           :collapse (clojure.core/or collapse false)})
             (size g {:axis axis})))))))

(defn normalize
  "Calculates baseline-corrected-g / variance, where baseline-correcred-g is g - mean(g), and variance is
  sum(base-line-corrected-g^2) / size(baseline-corrected-g). Options can include `:axis`, a single axis or
  a vector of axes over which to perform the `sum`, `size`, and `mean` operations. If `:axis` is not
  provided, normalization will be done over the entire tensor `g`. The returned result is the same shape as
  `g`."
  ([g]
   (normalize g {}))
  ([g options]
   (u/check-key-validity options [] [:axis])
   (let [axis (get options :axis 0)
         opts {:axis axis
               :collapse false}
         mean (mean g opts)
         baseline-corrected (G (- g mean))
         variance (G (/ (sum-along (pow baseline-corrected 2) opts)
                        (size baseline-corrected {:axis axis})))]
     (create-layer
       (gen-layer-name :normalize)
       [g]
       (G (/ baseline-corrected
             variance))))))

(defn relu
  "Computes `min(g, 0)`."
  [g]
  (create-layer
    (gen-layer-name :relu)
    [g]
    (G (max g 0))))

(defn logistic
  "Computes `1 / (1 + e^-g)`."
  [g]
  (G (/ 1 (+ 1 (exp (- g))))))

(defn logistic-binary-classifier
  "Returns a boolean tensor representing `1 / (1 + e^-`g`) >= 0.5`."
  [g]
  (G (>= (logistic g) 0.5)))

(defn dense
  "Given an `operand` matrix of shape [m input-size], computes `operand . W + b`, where `.` is matrix
  multiplication, W is a parameter matrix of shape [input-size units], and b is a parameter matrix of shape
  [1 units]. Options include:

    - `:units` (required) - scalar number of 'units' to include in the layer
    - `:initializer` (optional, default xavier normal) - single initializer for both W and b
    - `:W-intializer` (optional) - initializer for W, used instead of that specified in `:initializer` if
       both are provided (default xavier normal)
    - `:b-initializer` (optional) - like `:W-initializer` but for b (default zeros)
    - `:regularizer` (optional) - default `nil`"
  [operand options]
  (u/check-key-validity options [:units] [:initializer :W-initializer :b-initializer :regularizer])
  (let [{:keys [units regularizer initializer W-initializer b-initializer]} options
        W-initializer (get-layer-initializer (or W-initializer initializer :xavier-normal) 0 1 :dense)
        b-initializer (get-layer-initializer (or b-initializer initializer :zeros) 0 1 :dense)
        input-node (r/construct-operand operand)
        layer-name (gen-layer-name :dense)
        input-dimension (when (r/is-const? input-node)
                          (m/column-count (r/get-node-value input-node)))
        input-name (r/get-node-name input-node)
        W-initializer-fn (fn [input-map]
                           (W-initializer [(or input-dimension
                                               (m/column-count (get input-map input-name)))
                                           units]))
        b-initializer-fn (fn [input-map]
                           (b-initializer [1 units]))
        W (make-parameter-node :W layer-name W-initializer-fn {:regularizer regularizer})
        b (make-parameter-node :b layer-name b-initializer-fn)]
    (create-layer
      layer-name
      [input-node]
      (G (+ (mmul input-node W) b)))))

(r/defop- pool2d
  "Given tensor `nd` of shape [m, height, width, channels], vector `pool-size` of shape
  [pool-rows, pool-columns], and vector `strides` of shape [vstride, hstride], returns a tensor of shape
  [m, floor((height - pool-rows) / vstride) + 1, floor((width - pool-cols) / hstride) + 1,
  pool-rows * pool-columns * channels]. In the returned tensor, the vector [k, i, j, :all] (i.e. the elements
  along the last dimension at a given point), consists of the elements of `nd` in selection
  [i * vstride...(i * vstride + pool-rows - 1), j * hstride...(j * hstride + pool-cols - 1), :all-channels]
  linearized in order row, col, channel. That is, channel changes fastest, and row changes slowest."
  [^:nd nd pool-size strides]
  :v (let [[kh kw] pool-size
           [m height width chs] (m/shape nd)
           [vstride hstride] strides
           unstrided-result-height (inc (- height kh))
           unstrided-result-width (inc (- width kw))]
       (when (or (zero? unstrided-result-height)
                 (zero? unstrided-result-width))
         (u/throw-str "Cannot perform 2d convolution of a tensor of shape '" (m/shape nd) "' with a kernel of "
                      "shape '" pool-size "'. The kernel width and height cannot exceed the width and "
                      "height of an input tensor."))
       (->> (for [i (range kh)
                  j (range kw)]
              (m/select-range nd
                              :all
                              [i (+ unstrided-result-height i) vstride]
                              [j (+ unstrided-result-width j) hstride]
                              :all))
            (apply m/join-along 3)))
  :dnd (let [[kh kw] pool-size
             nd-shape (m/shape nd)
             [_ height width chs] nd-shape
             unstrided-result-height (inc (- height kh))
             unstrided-result-width (inc (- width kw))
             [vstride hstride] strides
             result (m/zeros nd-shape)]
         (doseq [i (range kh)
                 j (range kw)
                 :let [k (* chs (+ (* i kw) j))]]
           (m/add! (m/select-range result
                                   :all
                                   [i (+ unstrided-result-height i) vstride]
                                   [j (+ unstrided-result-width j) hstride]
                                   :all)
                   (m/select-range dv :all :all :all [k (+ k chs)])))
         result))

; TODO: Research other possible implementations of this to see if this can be made faster. (e.g. doing
; it in frequency space).
(defn conv2d
  "Given tensor `nd` of shape [m, height, width, channels], a `kernel` of shape
  [output-chs, kernel-height, kernel-width, input-chs], and a vector of strides like `[row-stride,
  column-stride]`, returns a tensor of shape [m, floor((height - kernel-height + 1) / row-stride),
  floor(width - kernel-width + 1) / column-stride), output-chs] that is the product of convolving each
  of m examples with the kernel."
  [nd kernel strides]
  (let [[kout_chs kh kw] (mapv #(G (size kernel {:axis %})) (range 3))
        pooled-nd (G (pool2d nd (make-shape kh kw) strides))
        [m height width linear-kernel-size] (mapv #(G (size pooled-nd {:axis %})) (range 4))
        reshaped-kernel (G (transpose (reshape kernel (make-shape kout_chs linear-kernel-size))))]
    ; We reshape `pooled-nd` into a matrix, before calculating the product with `reshaped-kernel`, because
    ; backpropagation of `mmul` is much faster for matrices than for tensors.
    (G (-> (reshape pooled-nd (make-shape (* m height width) linear-kernel-size))
           (mmul reshaped-kernel)
           (reshape (make-shape m height width kout_chs))))))

(defn- get-2d-padding
  "Given a padding option accepted by a 2d pooling or convolutional layer, returns a full padding vector for
  a 4d tensor accepted by that layer. Throws an exception if `padding-option` is not valid. See `conv2d` and
  `max-pooling2d`."
  [padding-option window-size]
  (let [padding-dimensionality (u/vector-dimensionality padding-option)]
    (cond (nil? padding-option) nil
          (= :same padding-option) (let [[padding-h padding-w] (mapv #(/ (dec %) 2) window-size)]
                                     [[0 0]
                                      [(int (Math/ceil padding-h)) (int (Math/floor padding-h))]
                                      [(int (Math/ceil padding-w)) (int (Math/floor padding-w))]
                                      [0 0]])
          (number? padding-option) [0 padding-option padding-option 0]
          (= padding-dimensionality 1) [0 (first padding-option) (second padding-option) 0]
          (= padding-dimensionality 2) [[0 0] (first padding-option) (second padding-option) [0 0]]
          :else (u/throw-str "Invalid 2d padding: '" padding-option "'. Padding must be a valid argument to "
                             "`pad-with` for a matrix."))))

(defn max-pooling2d
  "Returns a new tensor containing the maximum elements of `nd` in a window sliding along two dimensions.
  `nd` must a tensor of shape [m, height, width, channels]. Options include:

    - `:pool-size` (required) - [height, width] of the sliding window
    - `:strides` (optional, defaults to `:pool-size`) - the [vertical, horizontal] step sizes to use when
      sliding the window.
    - `:padding` (optional) - an argument accepted by `pad-with`.

  The returned tensor is of shape [m, floor((height + vertical-padding - pool-height + 1) / vertical-stride),
  floor((width + horizontal-padding - pool-width + 1) / horizontal-stride), channels]. For a given slice
  through the first dimension (typically a training example) and channel, each (i, j) in the result is the
  maximum element in ((k * vertical-stride)...(k * vertical-stride + pool-height),
  (l * horizontal-stride)...(l * horizontal-stride + pool-width)) from `nd`."
  [nd options]
  (u/check-key-validity options [:pool-size] [:strides :padding])
  (let [{:keys [pool-size strides padding]} options
        _ (when-not (and (= 2 (count pool-size)))
            (u/throw-str "Invalid pool size provided to `max-pooling2d`: '" pool-size "'. Pool size must be "
                         "a two-element vector."))
        padding (get-2d-padding padding pool-size)
        padded-input (if padding
                       (G (pad nd padding))
                       nd)
        strides (or strides pool-size)
        _ (when-not (= (count strides) 2)
            (u/throw-str "Invalid :strides provided to `max-pooling2d`: '" strides "' . Strides must be a "
                         "2-element seq."))
        pool-element-count (apply * pool-size)
        pooled (pool2d padded-input pool-size strides)
        pooled-height (G (size pooled {:axis 1}))
        pooled-width (G (size pooled {:axis 2}))
        m (G (size nd {:axis 0}))
        chs (G (size nd {:axis 3}))]
    (G (-> pooled
           (reshape (make-shape m pooled-height pooled-width pool-element-count chs))
           (max-along {:axis 3 :collapse true})))))

; Padding can be :same, a vector of paddings for each dimension, or a scalar
(defn conv2d-layer
  "Returns the 2d convolution of `operand` with kernels of specified properties. `options` include:

    - `:kernel-size` (required) - A two-element vector specifying the kernel [height width].
    - `:output-channels` (required) - Number of output channels (equal to the number of kernels that will be
      convolved with `operand`).
    - `:strides` (optional, default [1 1]) - The [vertical horizontal] step sizes to use when sliding the
      kernel over `operand`.
    - `:initializer` (optional, default xavier normal) - used for both the kernel and the bias if
      initializers specific to those parameters are not given
    - `:kernel-iniitializer` (optional)
    - `:bias-initializer` (optional)
    - `:padding` (optional) - an argument accepted by `pad-with`. `operand` is padded with zeros before
      convolution.

  See `conv2d` for the structures of the input and output tensors."
  [operand options]
  (u/check-key-validity options [:kernel-size :output-channels] [:strides :padding :regularizer :initializer
                                                                 :kernel-initializer :bias-initializer])
  (let [{:keys [kernel-size output-channels strides initializer padding regularizer
                kernel-initializer bias-initializer]} options
        _ (when-not (and (= 2 (count kernel-size)))
            (u/throw-str "Invalid kernel provided to conv2d layer: '" kernel-size "'. Kernel size must be "
                         "a two-element vector."))
        padding (get-2d-padding padding kernel-size)
        kernel-initializer (get-layer-initializer (or kernel-initializer initializer :lecun-normal)
                                                  [1 2 3] nil :conv2d)
        bias-initializer (get-layer-initializer (or bias-initializer initializer :zeros) nil nil :conv2d)
        strides (or strides [1 1])
        _ (when-not (= (count strides) 2)
            (u/throw-str "Invalid :strides provided to `conv2d`: '" strides "' . Strides must be a 2-element "
                         "seq."))
        input-node (r/construct-operand operand)
        input-name (r/get-node-name input-node)
        kernel-initializer-fn (fn [input-map]
                                (let [input-channels (m/dimension-count (get input-map input-name) 3)]
                                  (kernel-initializer (concat [output-channels]
                                                              kernel-size
                                                              [input-channels]))))
        bias-initializer-fn (fn [_] (bias-initializer [1 1 1 output-channels]))
        layer-name (gen-layer-name :conv2d)
        kernel (make-parameter-node :kernel layer-name kernel-initializer-fn {:regularizer regularizer})
        bias (make-parameter-node :bias layer-name bias-initializer-fn)
        padded-input (if padding
                       (G (pad input-node padding))
                       input-node)]
    (create-layer
      layer-name
      [input-node]
      (G (+ (conv2d padded-input kernel strides) bias)))))

(defn softmax
  ([operand]
   (softmax operand {}))
  ([operand options]
   (u/check-key-validity options [] [:axis])
   (let [axis (:axis options)
         ; Subtracting the max before exponentiating prevents overflow. See Goodfellow, Bengio, and Courville
         ; (2016), pg. 179.
         max-corrected (G (->> (max-along operand {:axis axis :collapse false})
                               (- operand)
                               exp))]
     (G (/ max-corrected
           (sum-along max-corrected {:axis axis :collapse false}))))))

(defn cross-entropy
  "Computes `-esum(actual * log (predicted + eps))`, where `esum` is the sum over all elements and `eps` is
  a small value added for numerical stabilitiy."
  [predicted actual]
  (let [epsilon (m/array 1e-7)]
    (G (-> (+ predicted epsilon)
           log
           (* actual)
           esum
           negate))))

(defn binary-cross-entropy
  "Computes binary cross entropy:
  `-esum(actual * log (predicted + eps) + (1 - actual) * log (1 - predicted + eps)`, where `esum` is the sum
  over all elements and `eps` is a small value added for numerical stabilitiy.`"
  [predicted actual]
  (G (+ (cross-entropy predicted actual)
        (cross-entropy (- 1 predicted) (- 1 actual)))))

(defn recurrent
  "Simple recurrent layer. `operand` must be a tensor of shape `[example-count example-size timepoint-count]`.
  Required options are:

    - `:hidden-units` - size of hidden state
    - `:output-units` - output dimensionality

  Additional options are:

    - `:initializer` - weight/bias initializer for the internal `dense` layer
    - `:W-initializer` - weight initializer for the internal `dnese` layer
    - `:b-initializer` - bias initializer for the internal `dense` layer
    - `:state-initializer` - initializer for the hidden state matrix
    - `:regularizer` - regularizer for output and recurrent dense layer (fallback if `:output-regularizer`
      or `:recurrent-regularizer` isn't provided)
    - `:output-regularizer` - regularizer for the output `dense` layer
    - `:recurrent-regularizer` - regularizer for the recurrent `dense` layer
    - `recurrent-activation` - activation function for the recurrent connection
    - `:stateful` (default `false`) - boolean indicating if state should be maintained between iterations
    - `:mask` - boolean matrix of shape `[example-count timepoint-count]`. If provided, then state and output
      for a given example is updated at all timepoints where the `:mask` is true (1.0). Where the `:mask` is
      false (0.0), the previous state and output value are maintained.

  The output is of shape `[example-count output-units timepoint-count]`."
  [operand options]
  (u/check-key-validity options [:hidden-units :output-units]
                        [:initializer :W-initializer :b-initializer :state-initializer :regularizer
                         :output-regularizer :recurrent-activation :recurrent-regularizer :stateful
                         :mask])
  (let [input-node (r/construct-operand operand)
        input-node-name (r/get-node-name input-node)
        num-units (:hidden-units options)
        recurrent-activation (or (:recurrent-activation options) identity)
        state-initializer (get-layer-initializer (or (:state-initializer options) :zeros) nil 0 :recurrent)
        state-initializer-fn (fn [input-map]
                               (let [num-examples (first (m/shape (get input-map input-node-name)))]
                                 (state-initializer [num-examples num-units])))
        layer-name (gen-layer-name :recurrent)
        state (if (:stateful options)
                (make-state-node :state layer-name state-initializer-fn)
                (make-input-node :state layer-name state-initializer-fn))
        recurrent-regularizer (or (:regularizer options) (:recurrent-regularizer options))
        output-regularizer (or (:regularizer options) (:output-regularizer options))
        timepoint-input (make-input-node :timepoint-input layer-name)
        mask (when-let [mask (:mask options)]
               (r/construct-operand mask))
        mask-input (make-input-node :mask layer-name)
        updated-state (G (-> (join-along 1 state timepoint-input)
                             (dense (assoc (select-keys options [:initializer :W-initializer :b-initializer])
                                      :units (:hidden-units options)
                                      :regularizer recurrent-regularizer))
                             recurrent-activation))
        next-state (G (+ (* mask-input updated-state)
                         (* (- 1 mask-input) state)))]
    (create-layer
      layer-name
      (keep identity [input-node mask])
      (G (fork [timepoint-input (slices input-node 2)
                mask-input (if mask
                             (partition-along (r/construct-operand mask) 1)
                             (-> input-node (size {:axis 2}) make-shape ones slices))]
               [state state]
               (dense next-state {:units (:output-units options)
                                  :regularizer output-regularizer})
               [next-state]
               {:output-time-axis 2})))))

(defn gru
  "Gated recurrent unit layer. Input, output, and options are the same as for a `recurrent` layer, with
  the following additional options:

    - `:relevance-gate-initializer`
    - `:relevance-gate-regularizer`
    - `:update-gate-initializer`
    - `:update-gate-regularizer`"
  [operand options]
  (u/check-key-validity options [:hidden-units :output-units]
                        [:initializer :W-initializer :b-initializer :state-initializer
                         :recurrent-activation :relevance-gate-initializer :update-gate-initializer
                         :regularizer :relevance-gate-regularizer :update-gate-regularizer
                         :recurrent-regularizer :output-regularizer :stateful :mask])
  (let [input-node (r/construct-operand operand)
        input-node-name (r/get-node-name input-node)
        num-units (:hidden-units options)
        layer-name (gen-layer-name :gru)
        recurrent-activation (or (:recurrent-activation options) identity)
        state-initializer (get-layer-initializer (or (:state-initializer options) :zeros) nil 0 :gru)
        state-initializer-fn (fn [input-map]
                               (let [num-examples (first (m/shape (get input-map input-node-name)))]
                                 (state-initializer [num-examples num-units])))
        state (if (:stateful options)
                (make-state-node :state layer-name state-initializer-fn)
                (make-input-node :state layer-name state-initializer-fn))
        dense-options (assoc (select-keys options [:initializer :W-initializer :b-initializer])
                        :units (:hidden-units options))
        relevance-gate-regularizer (or (:regularizer options) (:relevance-gate-regularizer options))
        update-gate-regularizer (or (:regularizer options) (:update-gate-regularizer options))
        recurrent-regularizer (or (:regularizer options) (:recurrent-regularizer options))
        output-regularizer (or (:regularizer options) (:output-regularizer options))
        timepoint-input (make-input-node :timepoint-input layer-name)
        state-input (G (join-along 1 state timepoint-input))
        relevance-gate (G (-> state-input
                              (dense (assoc dense-options :regularizer relevance-gate-regularizer))
                              logistic))
        mask (when-let [mask (:mask options)]
               (r/construct-operand mask))
        mask-input (make-input-node :mask layer-name)
        ; num-examples x num-hidden-units
        update-gate (G (-> state-input
                           (dense (assoc dense-options :regularizer update-gate-regularizer))
                           logistic
                           (* mask-input)))
        candidate-next-state (G (-> (join-along 1 (* relevance-gate state) timepoint-input)
                                    (dense (assoc dense-options :regularizer recurrent-regularizer))
                                    recurrent-activation))
        next-state (G (+ (* update-gate candidate-next-state)
                         (* (- 1 update-gate) state)))]
    (create-layer
      layer-name
      (keep identity [input-node mask])
      (G (fork [timepoint-input (slices input-node 2)
                mask-input (if mask
                             (partition-along (r/construct-operand mask) 1)
                             (-> input-node (size {:axis 2}) make-shape ones slices))]
               [state state]
               (dense next-state {:units (:output-units options)
                                  :regularizer output-regularizer})
               [next-state]
               {:output-time-axis 2})))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; NN-specific reporters
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

(defn make-multiclass-accuracy-reporter
  "Returns a reporter function for the accuracy of predictions for a multi-class classification problem.
  The returned function receives arguments [predicted actual], where `predicted` is a tensor containing
  probabilities for each class along one axis and samples along all other axes. `actual` is a binary tensor
  of the same structure containing the true class memberships represented in one-hot form. `options` can
  include:

    - `:class-axis`: the index of the axis spanning classes"
  ([]
   (make-multiclass-accuracy-reporter {:class-axis 1}))
  ([options]
   (u/check-key-validity options [:class-axis] [])
   (let [class-axis (:class-axis options)]
     (fn [predicted actual]
       #_(println "predicted:" (first (m/slices predicted)))
       #_(println "actual: " (first (m/slices actual)))
       (-> predicted
           (r/tensure-max-mask class-axis)
           (m/mul actual)
           m/esum
           m/scalar->number
           (/ (apply * (assoc (m/shape predicted) class-axis 1)))
           (* 100)
           (r/make-datapoint "Accuracy" :percent))))))

; TODO: Test this (this has not been tested at all)
(defn make-binary-classifier-reporter
  "Returns a reporter function for the accuracy of predictions for a binary classification problem. The
  returned function receives arguments [predicted actual], where `predicted` is a tensor where each element
  is a probability of that sample belonging to one class, and `actual` is a binary tensor of the same shape
  representing the true class memberships."
  []
  (fn [predicted actual]
    (-> (m/ge predicted (m/array 0.5))
        (m/eq actual)
        m/esum
        m/->int
        (/ (m/ecount actual))
        (* 100.0)
        (r/make-datapoint "Accuracy" :percent))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; Model
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

(def reserved-node-names #{:training :predicting :testing})

(defn make-model
  "Cosntructs a model, given:

    - `:data` (required) - A map of maps (dataset-name -> variable-name -> data). The common case is to have
      multiple datasets for different uses (e.g. `:training`, `:dev`, and `:test`), each with the same
      variables (e.g. `:x` and `:y`).
    - `:graph` (required) - A computation graph for the model.
    - `:optimizer` (optional, defaults to gradient-descent-optimizer)
    - `:optimizer-options` (optional) - Map of options to pass to `:optimizer`
    - `:constants` (optional) - A map of name to value for graph inputs that do not change (i.e. that are
      constant cross datasets).
    - `:parameters` (optional) - A map of name to value for graph parameters. This can be excluded if all
      parameters have initialization functions specified."
  [& {:as options}]
  (u/check-key-validity options [:graph :data] [:constants :parameters :optimizer :optimizer-options
                                                ; This is a private piece of data used to keep track of
                                                ; prior optimization results.
                                                ::prev-optimizer-results])
  (let [{:keys [graph constants parameters optimizer optimizer-options data]
         :or {constants {}
              parameters {}
              optimizer r/gradient-descent-optimizer
              optimizer-options {}}} options
        all-input-keys (into #{} (mapcat keys (vals data)))
        ;; data (u/update-vals data (fn [dataset]
        ;;                            (u/update-vals dataset #(if (m/array? %) % (m/array %)))))
        ]
    (when-let [invalid-input-names (seq (set/intersection reserved-node-names all-input-keys))]
      (u/throw-str "Invalid input names: '" invalid-input-names "'. These names are reserved."))
    (when-let [invalid-constant-names (seq (set/intersection reserved-node-names
                                                             (into #{} (keys constants))))]
      (u/throw-str "Invalid constant names: '" invalid-constant-names "'. These names are reserved."))
    {:graph graph
     :constants constants
     :parameters parameters
     :optimizer optimizer
     :optimizer-options optimizer-options
     :data data
     ::prev-optimizer-results (get options ::prev-optimizer-results {})}))

(defn update-model
  [model & {:as updates}]
  (apply make-model (apply concat (merge model updates))))

(defn update-prev-optimizer-results
  [model dataset-or-name optimizer-result]
  (assoc-in model [::prev-optimizer-results dataset-or-name] optimizer-result))

(defn get-previous-optimizer-result
  [model dataset-or-name]
  (get-in model [::prev-optimizer-results dataset-or-name]))

(defn get-model-constants
  [model]
  (:constants model))

(defn get-model-parameters
  [model]
  (:parameters model))

(defn get-model-optimizer
  [model]
  (:optimizer model))

(defn get-model-optimizer-options
  [model]
  (:optimizer-options model))

(defn get-model-graph
  "Returns `model`'s computation graph."
  [model]
  (:graph model))

(defn get-model-dataset
  "If `dataset-or-name` is a map (assumed to contain variable-name -> value), just returns the map;
  otherwise looks up the dataset by the provided name in the model's associated data and returns the
  corresponding map of variable-name -> value."
  [model dataset-or-name]
  (if (map? dataset-or-name)
    dataset-or-name
    (let [data (:data model)
          dataset (get data dataset-or-name)]
      (when-not dataset
        (u/throw-str "Could not locate dataset '" dataset-or-name "'. Model only has datasets: '"
                     (keep (fn [[k v]] (when v k)) data) "'."))
      dataset)))

(defn get-model-data
  "Returns data for a given dataset and variable name. For example, `(get-model-data :training :x)`. Throws
  exceptions if the model doesn't have a dataset or variable of the given name."
  [model dataset-name varname]
  (let [dataset (get-model-dataset model dataset-name)
        var-data (get dataset varname)]
    (when-not var-data
      (u/throw-str "Could not locate data for variable '" varname "' in dataset '" dataset-name
                   "'. Dataset only has variables: '" (keep (fn [[k v]] (when v k)) dataset) "'."))
    var-data))

(defn get-model-input-map
  "Returns all a map of all model graph inputs (constants and vars from `dataset-or-name`)."
  [model dataset-or-name]
  (merge (get-model-dataset model dataset-or-name)
         (get-model-constants model)))

(defn- get-all-model-parameter-nodes
  "Returns a seq of all distinct input nodes in `model`'s graph that are parameters (either have `:parameter`
  in their data set to `true` or are in the model`s `:parameters` map."
  [model]
  (let [specified-params (into #{} (keys (get-model-parameters model)))]
    (->> (get-model-graph model)
         r/get-all-graph-inputs
         distinct
         (filter #(or (-> % r/get-node-data :parameter)
                      (specified-params (r/get-node-name %)))))))

(defn- get-all-model-parameter-names
  "Like `get-all-model-parameter-nodes` but returns a seq of names instead of a seq of nodes."
  [model]
  (map r/get-node-name (get-all-model-parameter-nodes model)))

(defn- get-all-model-parameter-values
  "Returns a map of parameter name->value for all parameters of the given graph and dataset. Will include
  `nil` values for parameters that have not been initialized."
  [model]
  (let [graph (get-model-graph model)
        parameters (get-model-parameters model)
        uninitialized-parameter-names (set/difference (into #{} (get-all-model-parameter-names model))
                                                      (into #{} (keys parameters)))
        uninitialized-parameter-map (->> (map (juxt identity (constantly nil)) uninitialized-parameter-names)
                                         (into {}))]
    (merge parameters
           uninitialized-parameter-map)))

(defn- get-trainable-parameter-name-set
  [model]
  (->> (get-all-model-parameter-nodes model)
       (keep #(when (:trainable (r/get-node-data %))
                (r/get-node-name %)))
       (into #{})))

;; TODO: Get rid of `:training`, `:testing`, `:predicting` flags and just allow the user to pass these in
;; (e.g. either directly in the dataset, or through some option that allows additional inputs to be provided).
(defn evaluate-model
  "Returns the result of running `model` on `dataset-or-name`. For example,
  `(evaluate-model model :training)` will return the model's output for the `:training` dataset. Options
  include:

    - `:training` - boolean indicating if the `:training` input should be set to `true`
    - `:testing` - boolean indicating if the `:testing` input should be set to `true`

  If neither `:training` nor `:testing` is specified, the `:predicting` flag is set to true."
  ([model dataset-or-name]
   (evaluate-model model dataset-or-name {}))
  ([model dataset-or-name options]
   (u/check-key-validity options [] [:training :testing])
   (let [extra-inputs (cond (:training options) {:training 1 :predicting 0 :testing 0}
                            (:testing options) {:training 0 :predicting 0 :testing 1}
                            :else {:training 0 :predicting 1 :testing 0})
         graph (get-model-graph model)]
     (->> (merge (get-all-model-parameter-values model)
                 extra-inputs
                 (get-model-input-map model dataset-or-name))
          (r/evaluate graph)))))

(defn- make-optimized-model
  "Returns a new model derived from `model` but with all properties updated based on `optimizer-result`."
  [model dataset-or-name optimizer-result]
  (let [untrained-parameters (r/get-optimized-state optimizer-result)
        trained-parameters (r/get-optimized-params optimizer-result)]
    (-> (update-model model
                      :graph (r/get-optimized-graph optimizer-result)
                      :parameters (merge trained-parameters
                                         untrained-parameters))
        (update-prev-optimizer-results dataset-or-name (r/reset-optimizer-reporting optimizer-result)))))

(defn- optimize-model
  [model dataset-or-name iterations async?]
  (let [optimizer (get-model-optimizer model)
        optimizer-options (as-> (get-model-optimizer-options model) optimizer-options
                            (if (and #_(= optimizer r/gradient-descent-optimizer)
                                     (:batch-size optimizer-options)
                                     (not (:batch-inputs optimizer-options)))
                              (let [default-batch-inputs (->> (get-model-dataset model dataset-or-name)
                                                              keys
                                                              (mapv #(vector % 0))
                                                              (into {}))]
                                (assoc optimizer-options :batch-inputs default-batch-inputs))
                              optimizer-options)
                            (assoc optimizer-options :iterations iterations))
        optimize-fn (if async?
                      r/optimize-async
                      r/optimize)]
    (if-let [prev-result (get-previous-optimizer-result model dataset-or-name)]
      (optimize-fn optimizer prev-result optimizer-options)
      (let [graph (get-model-graph model)
            all-param-map (get-all-model-parameter-values model)
            trainable-param-name-set (get-trainable-parameter-name-set model)
            trainable-param-map (select-keys all-param-map trainable-param-name-set)
            state-param-map (apply dissoc all-param-map trainable-param-name-set)
            extra-inputs {:training 1
                          :predicting 0
                          :testing 0}
            input-map (merge extra-inputs
                             (get-model-input-map model dataset-or-name))]
        (optimize-fn optimizer graph input-map trainable-param-map state-param-map optimizer-options)))))

(defn train-model
  "Fits `model` to data in `dataset-or-name` by running the model's optimizer for `iterations` iterations.
  Defaults to running 1 iteration on the `:training` data. Blocks until complete."
  ([model]
   (train-model model 1))
  ([model iterations]
   (train-model model :training iterations))
  ([model dataset-or-name iterations]
   (let [optimizer-result (optimize-model model dataset-or-name iterations false)]
     (make-optimized-model model dataset-or-name optimizer-result))))

(defn train-model-async
  "Fits `model` to data in `dataset-or-name` (default `:training`) by running the model's optimizer for
  `iterations` (defaults to infinite). Launches optimization in a new thread and immediately returns a
  0-arity function, `stop`, that when called will immediately return the trained model from the last complete
  iteration and terminate the optimization process after the next iteration is complete."
  ([model]
   (train-model-async model nil))
  ([model iterations-or-dataset-or-name]
   (let [[iterations dataset-or-name] (if (or (nil? iterations-or-dataset-or-name)
                                              (number? iterations-or-dataset-or-name))
                                        [iterations-or-dataset-or-name :training]
                                        [nil iterations-or-dataset-or-name])]
     (train-model-async model dataset-or-name iterations)))
  ([model dataset-or-name iterations]
   (let [stop-optimizing (optimize-model model dataset-or-name iterations true)]
     (fn stop
       []
       (let [optimizer-result (stop-optimizing)]
         (make-optimized-model model dataset-or-name optimizer-result))))))

; TODO:
; Improvements
; - Remove `data` from model. It makes more sense to keep the model and data completely separate.
;   - This will also simplify all the `dataset-or-name` arguments where the supplied data can be either a
;     map of data or a dataset name.
;   - Could add a separate set of functions for managing datasets (e.g. bundling together sets of training
;     and test data as the model does now).
;   - Consider adding a set of required input names to model so datasets can be validated to ensure they have
;     the required inputs (probably not possible to validate that _all_ inputs exist due to `if` branches--
;     some graphs might not require some inputs, depending on conditional branching).
;   - Add validation to make sure regularization is applied (an easy mistake is to pass a regularizer
;     as an option for some layer and then to not pass the cost node through regularization layer). We can
;     check that if any nodes have regularization, there must be a `regularize` node and, if not, we can
;     throw an Exception. It's difficult to automatically inclue the `regularize` node, because it's
;     difficult to know where in the graph it should go if there are conditional branches.
;   - Figure out better way to handle print logging -- right now it's necessary to manually flush output
;     if the last logging statement doesn't have a newline (which triggers a flush). Somehow always flush the
;     print logger after the last iteration.
;   - Be able to lazily load batches into memory.
;   - Allow simple-optimize-like options for the model's optimizer-options

; Additional features
; - Serialization - be able to save and load models from/to disk
; - Periodic backups - be able to persist model data during training periodically, so if training gets
;   interrupted, not all progress is lost.
; - More layers
;     - Mean squared error (e.g. used in book reviews example)

; Quality
; - Tests:
;     - to ensure regularization is not applied more than once for each regularized node (this was a
;       previous bug that has since been fixed, but tests were not added to catch it).
;     - of one-hot encoding functions
;     - of softmax layer
;     - of cross-entropy layer
;     - of complete reporting and logging framework
;     - of the layer initialization framework
; - Figure out how to handle evaluation with stateful recurrent layers when batching. It is currently
;   possible to have batches that need state of different sizes, but if state is maintained between
;   iterations, this causes an excpetion. Remove state before evaluating?
; - Reset recurrent netowrk state between epochs?
; - Standardize names of arguments of the same type/meaning (e.g. sometimes "nd" is used for a tensor node,
;   and other times "operand" is used).
; - Automatically add '' around non-strings in throw-str
